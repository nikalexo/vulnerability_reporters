# Reporters
The scripts in this folder and its subfolders search for reporters of CVEs. The following order describes the steps of reporter fetching and processing.

## 1. Fetch reporters of CVEs from several sources
- **Folder:** CVE_2_Rep
- **Usage:** 
<pre>
python download_DSAs.py     # create mapping from dsa-id to html content as json 
python download_USNs.py     # create mapping from usn-id to html content as json 
python download_SF.py     # create mapping from sf-id to html content as json 
python download_RHs.py     # create mapping from RH BTS bug-id to html content as json 
python download_Android.py     # create mapping from android-id to html content as json 

bash download_extern_content.sh   # executes above scripts 

python get_all_reporters.py             # all projects
python get_all_reporters.py apache      # only apache
python get_all_reporters.py mozilla     # only mozilla
python get_all_reporters.py linux       # only linux
python get_all_reporters.py php         # only php
</pre>
- **Output:** Mappings from CVEs to a list of not cleaned reporters parsed from different sources. One file for each project
    - **Example:** "CVE-2005-1268": ["0", "Discovery is credited to Marc Stern.", "0", "0", "0", "0", "0"]. ("0" is equal to not found/available)
    - **List order (Sources):**  
        0: [Debian DSA](https://www.debian.org/security/),   
        1: [Securityfocus](https://www.securityfocus.com),  
        2: [Ubuntu USN](https://usn.ubuntu.com/),  
        3: [Redhat BTS](https://bugzilla.redhat.com/),  
        4: [Android bulletin](https://source.android.com/security/bulletin/),  
        5: [Mozilla Foundation Security Advisories (MFSA)](https://www.mozilla.org/en-US/security/advisories/) (mozilla only),  
        6: [Apache Security Reports](https://httpd.apache.org/) (apache only)
    - **Files:** reporters_{apache, mozilla, linux, php}.json 

## 2. Reporter processing and cleaning (automatic)
- **Folder:** Rep_2_Info_1
- **Usage:** 
<pre>
Reporter_postprocess.ipynb
</pre>
- **Output:** Mappings from reporters to the categories 'affiliations', 'cves', 'email addresses', and 'twitter accounts' parsed and cutted from the results of *get_all_reporters.py*. Two files per category (csv and json each) and one csv all in one file *rep2info.csv*.

## 3. Manual changes
At this point we did a manual cleaning process as described is 3.1.  
To use its results in step 4, follow the instructions in 3.2. 

### 3.1. Reporter processing and cleaning (manual)
- **Folder:** Rep_2_Info_2
- **Approach:** See *Rep_2_Info_2/rep2info_cleaning.md*
- **Output:** An improved file **rep2info_mod_6_1.csv=rep2info_s1.json** with mappings from reporters to information based on *rep2info.csv* mostly manually cleaned. The new category **type** was added, describing the type of reporters: team (t), company (c), person (p), undefined (u). The new category **alias** contain a list of aliases of the reporter.
- **Modifications:**
    - Fix encoding issues
    - Fix pseudonyms to real names
    - Remove words in names not belonging to it
    - Add all "not known" reporters to 
        - Multiple independent researchers
        - Anonymous
        - Not known

### 3.2. Including manual cleaning results
- **Folder:** Rep_2_Info_2
- **Usage:**
<pre>
Reporters_extend.ipynb
</pre>
It loads the file *Rep_2_Info_1/rep2info.csv* and add the new entries with a cve not yet in *Rep_2_Info_2/rep2info_s1.json* in this old mappings and stores the new complete mappings again into *Rep_2_Info_2/rep2info_s1.json*. To find equal reporters fuzzying is used. For all new reporters the type is set to "p", so regarded as a person. This should be manually cleaned to ensure higher accuracy in the experiments.  
Now you can do further manual cleaning steps as e.g. described in 3.1. which should again be saved in *Rep_2_Info_2/rep2info_s1.json*.

## 4. Hackerone Bug Bounty Program processing
- **Folder:** Rep_2_Info_3
- **Usage:**
<pre>
Hackerone_dump.ipynb
Hackerone_parse.ipynb
Hackerone_user_fetch.ipynb
Hackerone_2_cve.ipynb
</pre>
- **Output:** 
    - A new file **rep2info_s2.json** with new information about reporters and cves extracted from [Hackerone](https://hackerone.com).
    - Mappings from reporters to bounties: **h1_reporters.json**
    - Mappings from cves to bounties: **cve2h1**

## 5. Reporters from bugs
- **Folder:** Rep_2_Info_4
- **Usage**
<pre>
Reporters_from_bugs.ipynb
</pre>
- **Output:** A new file **rep2info_s3.json** with new information about reporters (e.g. email) and cves extracted from bugs.

## 6. Final cleaning
- **Folder:** Rep_2_Info_5
- **Usage:**
<pre>
Reporters_clean.ipynb  # Some cleaning steps rep2info
Reporters_final_cleaning.ipynb  # Some final cleaning steps rep2info
</pre>
- **Output:** A new file **rep2info_s4.json** with further cleaned information about reporters and cves. 
    - **Example json entry:** "ruben": {"cves": ["CVE-2015-4643", "CVE-2015-4022"], "affs": [], "emails": [], "twitter": [], "alias": [], "misc": [], "type": "p"}

## Bug bounties for experiments
- **Usage:**
<pre>
Bugs_bounties_all.ipynb         # Maps cve to bug bounties
</pre>
- **Output:** A mapping from cve to bug bounties (Hackerone, Bug bounty hints in bugs mapped to cves)

## Linkedin profiles for experiments
- **Requirements:** A *credentials.json* file with the login data for linkedin to be able to use the api. You can fill and rename *credentials.json.plain*.
- **Usage:**
<pre>
python linkedin.py              # Downloads profiles
python linkedin_rep2profile.py  # Creates rep2profiles.json
</pre>
- **Output:** A new file **Linkedin/rep2profiles.json** mapping reporters to linkedin profiles used in experiments.
- **Important:** Should a ChallengeException raise when running linkedin.py, then you maybe have to login into your linkedin profile with your browser first.

