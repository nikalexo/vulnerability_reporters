"""
Main script for downloading and parsing reporters from several sources
"""

import warnings
import requests
import json
import pandas as pd
# Custom utility functions shared across notebooks
import shelve
from pymongo import MongoClient
from bs4 import BeautifulSoup
import re
import html5lib
from bs4 import NavigableString
import pickle
from preprop import bs_preprocess

import sys
import traceback

# insert at 1, 0 is the script path (or '' in REPL)
sys.path.insert(1, './../../../cve-search/')

import lib.DatabaseLayer as db

# Load databases

client = MongoClient()
db1 = client.cvedb
collection1 = db1.cves
cve = collection1.find_one({"id": "CVE-2019-15891"})

if cve == '':
    logging.warning('CVE not found in mongodb')
    
def get_cves(cve_list):
    full_cves = []

    for cve in cve_list:
        item = collection1.find_one({"id": cve})
        if item != None:
            full_cves.append(item)

    return full_cves

# Get DSAs, securityfocus BIDs, USNs, BTSs, and Android Security Bulletins from CVE references
def get_refs(cve_list, num_sources):
    sources_cve = dict()
    
    # sources_cve = [dsa-id, securityfocus-bid, ubuntu sn, redhat bug report, android bulletin]
    
    for cve in cve_list:
        references = cve['references']
        temp = ['0']*num_sources
        sources_cve[cve['id']] = temp
        for ref in references:

            # First look into Debian. Get DSA or DLA from the references and look into local copy.
            
            url_dsa = 'www.debian.org/security/'
            to_search = r'((?<=' + url_dsa.replace('?', '\?').replace('=', '\=') + '([0-9]{4})/' + 'dsa-' + ')'
            to_search += ').+'
            m = re.search(to_search, ref)
            try:
                dsa_ref=m.group(0)
                temp[0] = dsa_ref
            except AttributeError:
                pass
        
            # Then look into SecurityFocus. Parse html and extract discoverer.

            url_sf = 'www.securityfocus.com/bid/'
            to_search = r'((?<=' + url_sf.replace('?', '\?').replace('=', '\=') + ')'
            to_search += ').+'
            m = re.search(to_search, ref)
            try:
                sf_ref=m.group(0)
                temp[1] = sf_ref
            except AttributeError:
                pass
        
            # Then look into Ubuntu. Get USN (remote).
        
            url_usn = 'usn.ubuntu.com/'
            to_search = r'((?<=' + url_usn.replace('?', '\?').replace('=', '\=') + ')'
            to_search += ').+'
            m = re.search(to_search, ref)
            try:
                usn_ref=m.group(0)
                temp[2] = usn_ref
            except AttributeError:
                url_usn = 'ubuntu.com/usn/'
                to_search = r'((?<=' + url_usn.replace('?', '\?').replace('=', '\=') + ')'
                to_search += ').+'
                m = re.search(to_search, ref)
                try:
                    usn_ref=m.group(0)
                    temp[2] = usn_ref
                except AttributeError:
                    pass
        
            # Then look into the Red Hat BTS
        
            url_rh = 'bugzilla.redhat.com/show_bug.cgi?id='
            to_search = r'((?<=' + url_rh.replace('?', '\?').replace('=', '\=') + ')'
            to_search += ').+'
            m = re.search(to_search, ref)
            try:
                rh_ref=m.group(0)
                temp[3] = rh_ref
            except AttributeError:
                url_rh = 'bugzilla.redhat.com/bugzilla/show_bug.cgi?id='
                to_search = r'((?<=' + url_rh.replace('?', '\?').replace('=', '\=') + ')'
                to_search += ').+'
                m = re.search(to_search, ref)
                try:
                    rh_ref=m.group(0)
                    temp[3] = rh_ref
                except AttributeError:
                    pass
            
            # It also makes sense to look into Red Hat security advisories? or get all the bugs from the BTS and search there for the CVE?
            
            # Then look into Android Security Bulletins (mostly for kernel)
        
            url_android = '/source.android.com/security/bulletin/'
            to_search = r'((?<=' + url_android.replace('?', '\?').replace('=', '\=') + ')'
            to_search += ').+'
            m = re.search(to_search, ref)
            try:
                android_ref=m.group(0)
                temp[4] = android_ref
            except AttributeError:
                pass
        
        sources_cve[cve['id']] = temp
    
    return sources_cve

# Extract reporters from DSAs html content with regular expresssions
def get_dsas(sources_cve, cve_list, reporters):
    
    # DSA reports from DVAF in that path
    with open('./dsa2info.json') as fp:
        dsa2info = json.load(fp)

    # This regex uses positive lookahead to try and match the discoverer
    regex = r'[a-zA-Z"](\w| |\'|"|-|,)+(?=( discovered)|( reported)|( found)|( noticed)|( provided a)|( resolved an))'
    exceptions = r'was$|been$|were$'
    cve_reg = r'CVE-.*'

    counter = 0
    
    for cve in cve_list:
        rep = ''
        dsa_id = sources_cve[cve['id']][0]
        if dsa_id == '0':
            continue
        else:
            try:
                if dsa_id == '047':
                    dsa_id = '47'
                temp = dsa2info[dsa_id]
                soup = BeautifulSoup(temp,"html.parser")
                # Match regex for discoverer
                for span in soup.select("li"):
                    text = span.text.split()
                    # This is to handle multiple CVEs in one attribution
                    for i in range(len(text)):
                        if text[i] == cve['id']:
                            j = 0
                            while re.match(cve_reg,text[i+1+j]):
                                j += 1
                            ttext = " ".join(text[i+1+j:])
                            m = re.search(regex, ttext)
                            try:
                                rep=m.group(0)
                                if re.search(exceptions, rep):
                                    continue
                                rep=re.sub(r'(CVE-\d+-\d+ )|(XSA-\d+ )','',rep)
                                rep=re.sub(r' has$','',rep)
                                #print(rep, dsa_id, cve['id'])
                                reporters[cve['id']][0]=rep
                                counter += 1
                            except AttributeError:
                                pass
            
            except KeyError:
                print('Cannot find DSA', dsa_id)
                continue
    
    print('Found', counter,'discoverers from DSA data')

# Extract reporters from USNs html content with regular expresssions
def get_usn(sources_cve, cve_list, reporters):

    # USN reports 
    with open('./usn2info.json') as fp:
        usn2info = json.load(fp)
    
    counter = 0

    # This regex uses positive lookahead to try and match the discoverer
    regex = r'[a-zA-Z"](\w| |\'|"|-)+(?=( discovered)|( reported)|( found)|( noticed)|( provided a)|( resolved an))'
    exceptions = r'was$|been$|were$'
    cve_reg = r'(?<=\()(CVE-(\d|-)*)(?=\))'

    for usn_id in usn2info:
        try:
            temp = usn2info[usn_id]
            soup = BeautifulSoup(bs_preprocess(temp),"html5lib")
            
            span = soup.find('h3',id='details')
            if span == None:
                for s in soup.find_all('h2'):
                    if "Details" == s.text:
                        span = s
                        break

            try:      
                span = span.nextSibling
            except:
                print(span, usn_id)

            while span != None and span.name=='p':
                cve = ''
                reps = ''
                rep=span.text
                #print(span)
                cve = re.search(cve_reg, rep)
                #print(cve)
                if cve == None:
                    span = span.nextSibling
                    continue
                #print(cve.group(0), usn_id)
            
                cve=cve.group(0)
                if cve not in reporters:
                    span = span.nextSibling
                    continue
                else:
                    reps = re.search(regex, rep)
                    if reps == None:
                        span = span.nextSibling
                        continue
                    else:
                        reps = reps.group(0)
                        if re.search(exceptions, reps):
                            span = span.nextSibling
                            continue
                        reps=re.sub(r'(CVE-\d+-\d+ )|(XSA-\d+ )','',reps)
                        reps=re.sub(r' has$','',reps)
                        reporters[cve][2]=reps
                        #print(reps)
                        counter += 1
                        span = span.nextSibling
            
        except AttributeError:
            print('Trouble extracting data from USN', usn_id)
            traceback.print_exc()
            continue
            
    print('Found', counter,'discoverers from USN data')

# Extract reporters from html content of securityfocus entries with regular expresssions
def get_sf(sources_cve, cve_list, reporters):
    counter = 0

    # SF reports 
    with open('./sf2info.json') as fp:
        sf2info = json.load(fp)

    for cve in cve_list:
        rep = ''
        sf_id = sources_cve[cve['id']][1]
        if sf_id == '0':
            continue
        else:
            try:
                temp = sf2info[sf_id]
                soup = BeautifulSoup(temp,"html.parser")
                span = soup.find('span',text='Credit:')
                span = span.find_parent('td')
                span = span.findNext('td').text
                rep=span
                #print(rep, sf_id)
                #print(counter)
                counter += 1
                reporters[cve['id']][1]=rep
            
            except:
                print('WARNING: Trouble with securityfocus bid ', sf_id)
                continue
    
    # Small pre-cleaning. We will clean more later
    
    for cve in sources_cve:
        reporters[cve][1]=re.sub(r'\n|\t|amp;','',reporters[cve][1])
        
    print('Found', counter,'discoverers from securityfocus data')

# Extract reporters from html content of red hat bts entries with regular expresssions
def get_rh(sources_cve, cve_list, reporters):
    
    with open("./rh_bugs.json") as jf:
        rh_bugs = json.load(jf)
    
    count = 0
    for cve in cve_list:
        try:
            for comment in rh_bugs[cve['id']]['bugs'][0]['comments']:
                text=comment['text']

                #m0 = re.search(r'cknowledgement|knowledgment', text)
                
                # The following regex has to change to accomodate text like: The upstream acknowledges... To be fixed in a future version
                
                m = re.search(r'(((?<=Red Hat would like to thank )|(?<=Red Hat would like to credit )).*\n?.*(?=( |\n)for))|((?<=This issue was discovered by ).*(?=.))|((?<=This issue was found by ).*(?=.))|((?<=Acknowledgments:\n\nName: ).*\n?.*(?=$))|((?<=Acknowledgments: \n\nName: ).*\n?.*(?=$))|((?<=Acknowledgements: \n\nName: ).*\n?.*(?=$))|((?<=Acknowledgements:\n\nName: ).*\n?.*(?=$))|((?<=Acknowledgment:\n\nName: ).*\n?.*(?=$))|(?<=The security impact of this issue was discovered by ).*(?=.$)', text)
                if m != None:
                    reporters[cve['id']][3]=m.group(0)
                    count += 1
        except KeyError:
            #print(cve['id'])
            continue
    
    print('Found', count,'discoverers from RedHat data')

# Extract reporters from html content of android bulletins entries with regular expresssions
def get_android(sources_cve, cve_list, reporters):
    counter = 0
    url_android = 'https://source.android.com/security/bulletin/'

    #unique_ids = []
    data_android = []

    # for cve in cve_list:
    #     and_id = sources_cve[cve['id']][4]
    #     if and_id == '0':
    #         continue
    #     else:
    #         if and_id not in unique_ids:
    #             unique_ids.append(and_id)

    with open("./android2info.json") as jf:
        android2info = json.load(jf)

    #for and_id in unique_ids:
    for and_id in android2info:
        try:
            temp = android2info[and_id]
            soup = BeautifulSoup(temp,"html5lib")
            span = soup.find('h2',id='acknowledgements')
            span2 = span.nextSibling.nextSibling.nextSibling.nextSibling
            if span2.name == 'table':
                span = span2
                for tag in span:
                    if tag.name == 'tr':
                        if (len(tag.contents)!=5):
                            print('CHECK Advisory: ',and_id)
                        i = 0
                        cves = ''
                        reps = ''
                        for line in tag:
                            if line.name != 'td':
                                continue
                            else:
                                if i == 0:
                                    cves = line.text
                                    i += 1
                                else:
                                    reps = line.text
                                    i = 0
                                    data_android.append([[cves], [reps]])   
            elif span2.name == 'ul':
                # Some older Advisories have a list
                span = span2
                for tag in span:
                    if tag.name == 'li':
                        text = tag.text
                        m0 = re.search(r'(.|\n)*(?=:)', text)
                        reps = m0.group(0)
                        m1 = re.search(r'(?<=:)(.|\n)*', text)
                        cves = m1.group(0)
                        data_android.append([[cves], [reps]])
            else:
                #print(span)
                print('WARNING: Some PROBLEM with Advisory: ', and_id)
            
            counter += 1
    
        except AttributeError:
            #print('Seems there are no acknowledgements in android advisory ', and_id)
            continue

    
    # For more recent years there is data on reporters in the Acknowledgements pages
    # Get the acknowledgements from https://source.android.com/security/overview/acknowledgements
    URL = 'https://source.android.com/security/overview/acknowledgements'
    temp = requests.get(URL)
    data_android_ack = []
    soup = BeautifulSoup(temp.text,"html5lib")
    tables = soup.findAll('table')

    for table in tables:
        for body in table:
            if body.name == 'tbody':
                for tag in body:
                    if tag.name == 'tr':
                        i = 0
                        cves = ''
                        reps = ''
                        for line in tag:
                            if line.name != 'td':
                                continue
                            else:
                                if i == 0:
                                    reps = line.text
                                    i += 1
                                else:
                                    cves = line.text
                                    i = 0
                                    data_android_ack.append([[cves], [reps]])

    count = 0
    for record in data_android_ack:
        cves = re.split(r',\s+|,\n\s+',record[0][0].strip())
        for cve in cves:
            if cve in sources_cve:
                reporters[cve][4] = record[1]
                count += 1
     
    count = 0
    for record in data_android:
        cves = re.split(r',\s+|,\n\s+',record[0][0].strip())
        for cve in cves:
            if cve in sources_cve:
                reporters[cve][4] = record[1]
                #print(reporters[cve][4])
                count += 1
    
    print('Found', count,'discoverers from Android data')

def test_coverage(cve_list, reporters, package):
    # This is just for testing coverage
    count_all=0
    count_ack=0
    for cve in cve_list:
        count_all += 1
        if reporters[cve['id']]!=['0']*7:
            count_ack += 1
        else:
            pass
            #print(cve['id'])
    print('Total number of vulnerabilities for ', package,':', count_all)
    print('Reporter data found for:', count_ack)
    percent = (float(count_ack)/count_all)*100
    print('Coverage: ' + '%.2f' % percent+' %')
    print('#'*80)


# package in [mozilla, apache, linux, php]   
def parse_reporters(package, cve_list):
    
    print('Starting finding reporters for ' + package + '. This may take a long time...')

    # Get all cves for the package with content of cve-search
    full_cves = get_cves(cve_list)

    print('Found', len(full_cves),'CVEs')

    # Verify no duplicates
    has_seen = []
    for item in full_cves:
        try:
            if item['id'] not in has_seen:
                has_seen.append(item['id'])
            else:
                print(item['id'])
        except TypeError:
            print(item)
    
    # Get references from NVD
    # The total number of sources is 7 at the moment
    num_sources = 7
    sources_cve = get_refs(full_cves, num_sources)
    
    # Initialize reporters
    reporters = dict()
    for cve in full_cves:
        reporters[cve['id']] = ['0']*num_sources
    
    # Get DSAs (Debian)
    get_dsas(sources_cve, full_cves, reporters)
    # Get USN (Ubuntu)
    get_usn(sources_cve, full_cves, reporters)
    # Get securityfocus.com-entries
    get_sf(sources_cve, full_cves, reporters)
    # Get Red Hat BTS
    get_rh(sources_cve, full_cves, reporters)
    # Get Android Security Bulletins (only for linux)
    if package == 'linux':
        get_android(sources_cve, full_cves, reporters)
    # Add entries from MFSA (only for mozilla)
    cve_ids_list = []
    for cve in full_cves:
        cve_ids_list.append(cve['id'])
    if package == 'mozilla-suite':
        count = 0
        with open('reps_mozilla.json', 'r') as fp:
            mozcve2rep = json.load(fp)
            for cve in mozcve2rep:
                if cve not in cve_ids_list:
                    print('ERROR! THIS COULD BE FATAL')
                else:
                    reporters[cve][5] = mozcve2rep[cve]
                    count += 1
        print('Found', count,'discoverers from Mozilla Foundation Security Advisories data')
    # Add entries from Apache Security Advisories
    if package == 'apache':
        count = 0
        with open('reps_apache.json', 'r') as fp:
            apcve2rep = json.load(fp)
            for cve in apcve2rep:
                if cve not in cve_ids_list:
                    print('ERROR! THIS COULD BE FATAL')
                else:
                    reporters[cve][6] = apcve2rep[cve]
                    count += 1
        print('Found', count,'discoverers from Apache Security Advisories data')
        
    # Test coverage
    test_coverage(full_cves, reporters, package)

    # Save reporters
    with open('reporters_' + package + '.json', 'w') as fp:
        json.dump(reporters, fp)
           
def parse_reporters_all(project2cves):
    for pkg in ['mozilla-suite', 'apache', 'linux', 'php']:
        parse_reporters(pkg, project2cves[pkg])

def parse_reporters_one(pkg, cve_list):
    parse_reporters(pkg, cve_list)