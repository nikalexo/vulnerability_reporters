"""
Download and parse apache http security advisories
"""
import requests
from bs4 import BeautifulSoup
import html5lib
import re
import json
import pickle
from preprop import bs_preprocess
import time

def parse_apache(cve_list):
    apacheurls = ['https://httpd.apache.org/security/vulnerabilities_24.html', 'https://httpd.apache.org/security/vulnerabilities_22.html',
             'https://httpd.apache.org/security/vulnerabilities_20.html', 'https://httpd.apache.org/security/vulnerabilities_13.html']
    
    # First Download all html pages
    
    html_text = []
    for URL in apacheurls:
        time.sleep(5)
        print('Trying URL: ',URL)
        temp = requests.get(URL)
        print('Reply: ',temp)
        html_text.append(temp)
    
    # Then parse them into a dict
    
    apachecve2reps = dict()
    prob_count = 0
    cvelist = []
    
    for temp in html_text:
        try:
            soup = BeautifulSoup(bs_preprocess(temp.text),"html5lib")
            span = soup.find_all('name')
            for item in span:
                if re.match(r'.*CVE-.*',item['name']):
                    cve = item['name']
                    if cve in cve_list:
                        tempspan = item.parent.parent.nextSibling
                        flag = False
                        for item in tempspan:
                            if re.match(r'.*Acknowledgements.*', item.text):
                                reps = item.text
                                flag = True
                        if not flag:
                            #print('Poblem with cve ', cve)
                            prob_count += 1
                        else:
                            apachecve2reps[cve]=reps
               
        except AttributeError:
            print('ERROR with URL', URL)
            continue
    
    # Post-processing of Acknowledgements field
    
    cve2reps = dict()
    for cve in apachecve2reps:
        text = apachecve2reps[cve]
        reps = re.search(r'(?<=issue was discovered by ).*|(?<=issue was reported by ).*|(?<=would like to thank ).*(?= (for|to).*reporting)|(?<=was discovered internally by ).*', text)
        if not reps:
            pass
            #print('Problem with ', cve)
            #print(repr(text))
        else:
            rep = reps.group(0)
            if rep[-1]=='.':
                rep=rep[:-1]
            cve2reps[cve] = reps.group(0)
            
    # Finally store the extracted data in the disk
    
    with open('reps_apache.json', 'w') as fp:
        json.dump(cve2reps, fp)

    print('APACHE Security Advisories: Extracted', len(cve_list), 'CVES with reporters for', len(cve2reps),'of them.')